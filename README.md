# VideoAnnotation

Project in which affective and emotional behaviors were hand-annotated from ECoG patient audio/videos and correlated with their respective neural data. The purpose of this project was to implement a number of machine learning practices to decode emotional affects (i.e. smiling, laughing, crying, pain, positive affect, emotional affect) from brain recorded intracranially. 

The MATLAB code involves plotting all of the behavioral/affective data, ranging from the emotional affect mentioned above alongside the activities a given patient engaged in (eating, drinking, watching TV, conversation, etc.). These data are presented in the form of box plots for all patients. 
